# –ù–∞–±–æ—Ä –î–∂–µ–Ω—Ç–ª—å–º–µ–Ω–∞ –ø–æ ML üòà

## –û—Å–Ω–æ–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã

### Linear Regression
- MSE, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ (Normal Equation), –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤
- –ü—Ä–æ–±–ª–µ–º–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏

**–í–æ–ø—Ä–æ—Å—ã:**
- –ß—Ç–æ —Ç–∞–∫–æ–µ MSE –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω?
- –ö–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏?
- –ö–æ–≥–¥–∞ —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –≤–º–µ—Å—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —É—Ä–∞–≤–Ω–µ–Ω–∏—è?
- –ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –≤ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏?
- –ß—Ç–æ —Ç–∞–∫–æ–µ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å –∏ –∫–∞–∫ —Å –Ω–µ–π –±–æ—Ä–æ—Ç—å—Å—è?
 
### Logistic Regression
- –°–∏–≥–º–æ–∏–¥–∞, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$
–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ (0, 1) - –∏–¥–µ–∞–ª—å–Ω–æ –¥–ª—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, –º–æ–Ω–æ—Ç–æ–Ω–Ω–∞—è, S –æ–±—Ä–∞–∑–Ω–∞—è

- Cross-entropy

$$
L(y, \hat{y}) = -[y\log(\hat{y}) + (1-y)\log(1-\hat{y})]
$$
y - –∏—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞,
$\hat{y}$ - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –º–µ—Ç–∫–∞.

- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π

$\sigma(z)$ - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–º—É –∫–ª–∞—Å—Å—É
$1 - \sigma(z)$ - –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–Ω–æ—Å—Ç–∏ –∫ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–º—É –∫–ª–∞—Å—Å—É

Odds (—à–∞–Ω—Å—ã):

$$
Odds = P(x=1\mid x) / P(y = 0 \mid x) = \sigma(z) / (1 - \sigma(x)) = e^{z}
$$
Log odds (–ª–æ–≥–∞—Ä–∏—Ñ–º —à–∞–Ω—Å–æ–≤):
$$
\text{Log-odds} = z = w_{0} + w_{1}x_{1} + \dots + w_{n}x_{n}
$$
- –ü—Ä–æ–±–ª–µ–º–∞ –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

**–í–æ–ø—Ä–æ—Å—ã:**
- –í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –ª–∏–Ω–µ–π–Ω–æ–π –∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π?
- –ü–æ—á–µ–º—É –≤ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–∏–≥–º–æ–∏–¥–∞?
- –ß—Ç–æ —Ç–∞–∫–æ–µ –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è?
- –ö–∞–∫ –ª–æ–≥—Ä–µ–≥ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏?

### Decision Tree
- –ö—Ä–∏—Ç–µ—Ä–∏–∏: Gini, Entropy

Gini impurity:
$$
Gini = 1 - \sum(p_{i})^{2}
$$
–î–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π:
$$
Gini = 1 - \sum_{i=1}^{c}(p_{i})^{2}
$$
–ì–¥–µ $p_{i}$ —ç—Ç–æ –¥–æ–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ –∫–ª–∞—Å—Å–∞ i –≤ —É–∑–ª–µ

–ü—Ä–∏–º–µ—Ä:
``` python
–£–∑–µ–ª —Å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ–º [30, 20] –æ–±—ä–µ–∫—Ç–æ–≤ –¥–≤—É—Ö –∫–ª–∞—Å—Å–æ–≤:
p‚ÇÅ = 30/50 = 0.6
p‚ÇÇ = 20/50 = 0.4
Gini = 1 - (0.6¬≤ + 0.4¬≤) = 1 - (0.36 + 0.16) = 0.48
```

–í–µ—Ä—Ö—É—à–∫–∞ –¥–µ—Ä–µ–≤–∞ (–° —É—Å–ª–æ–≤–∏–µ–º) –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è Root node –∏–ª–∏ –∂–µ The root.
–û—Å—Ç–∞–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è –Ω–∞–∑—ã–≤–∞—é—Ç Branches –∏–ª–∏ –∏–ª–∏ Internal nodes.
–í—Å–µ, —á—Ç–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç - Leaves or Leaf nodes.

–£ –Ω–∞—Å –µ—Å—Ç—å –∫–∞–∫–æ–π-—Ç–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º

| Loves Popcorn | Loves Soda | Age | Loves Cool As Ice |
| ------------- | ---------- | --- | ----------------- |
| Yes           | Yes        | 7   | No                |
| Yes           | No         | 12  | No                |
| No            | Yes        | 18  | Yes               |
| No            | Yes        | 35  | Yes               |
| Yes           | Yes        | 38  | Yes               |
| Yes           | No         | 50  | No                |
| No            | No         | 83  | No                |



```mermaid
graph TD
    A[Popcorn] -->|True| B[Cool As Ice]
    A -->|False| C[Cool As Ice]
    B -->|Yes| D[1]
    B -->|No| E[3]
    C -->|Yes| F[2]
    C -->|No| G[1]
```    


```mermaid
graph TD
    A[Soda] -->|True| B[Cool As Ice]
    A -->|False| C[Cool As Ice]
    B -->|Yes| D[3]
    B -->|No| E[1]
    C -->|Yes| F[0]
    C -->|No| G[3]
```    

Impure - –µ—Å–ª–∏ —Å–º–µ—Å—å Yes –∏ No —É –æ–¥–Ω–æ–≥–æ Leaf, –¥–æ–ø—É—Å—Ç–∏–º 3/1 —ç—Ç–æ impure, 0/3 - –Ω–µ—Ç.

Gini impurity –æ–¥–∏–Ω –∏–∑ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö, –Ω–æ –µ—â–µ –µ—Å—Ç—å Entropy, Information Gain.

$\text{Gini impurity for a Leaf} = 1 - \text{(probability of "Yes")}^{2} - \text{(probability of "No")}^{2}$

$Gini = 1-\left( \frac{1}{1+3} \right)^{2} - (\frac{3}{1+3})^{2}$

$\text{Total gini impurity = weighted average of gini impurities for the Leaves}$
$\text{Total gini} = \left( \frac{4}{4+3} \right)0.375 + (\frac{3}{4+3})0.444$

–í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–∞ –¥–ª—è —É—Å–ª–æ–≤–∏—è –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–æ–º —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Å—Ç–æ–≥–æ –¥–µ—Ä–µ–≤–∞ –ø—Ä–∏ –ø–æ–º–æ—â—å Gini Impurity –∏–ª–∏ Entropy (–í—ã–±–∏—Ä–∞–µ–º Min Impurity)

–≠–Ω—Ç—Ä–æ–ø–∏—è:
$$
Entropy = -\sum_{i=1}^{c} p_{i} \log_{2}(p_{i})
$$

``` python
–¢–æ—Ç –∂–µ —É–∑–µ–ª [30, 20]:
Entropy = -(0.6 √ó log‚ÇÇ(0.6) + 0.4 √ó log‚ÇÇ(0.4))
        = -(0.6 √ó (-0.737) + 0.4 √ó (-1.322))
        = -(-0.442 - 0.529) = 0.971
```

–≠–Ω—Ç—Ä–æ–ø–∏—è –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —Ç–æ—á–Ω–µ–µ, —Å–æ–∑–¥–∞–µ—Ç –±–æ–ª–µ–µ —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–µ—Ä–µ–≤—å—è.
Gini –ø—Ä–æ—â–µ –≤ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏, –∏–º–µ–µ—Ç —á—É—Ç—å —Ö—É–∂–µ —Ç–æ—á–Ω–æ—Å—Ç—å, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–µ–µ.

 ##### **Overfitting, Pruning**

- –°–ª–∏—à–∫–æ–º –≥–ª—É–±–æ–∫–∏–µ –¥–µ—Ä–µ–≤—å—è
- –°–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –ª–∏—Å—Ç–æ–≤—ã–µ —É–∑–ª—ã
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –Ω–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç—å
- –ó–∞—à—É–º–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

- Feature Importance



- Out-of-bag (–≤ RF)

**–í–æ–ø—Ä–æ—Å—ã:**
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π?
- –ß—Ç–æ –≤—ã–±—Ä–∞—Ç—å ‚Äî Gini –∏–ª–∏ Entropy?
- –ö–∞–∫ –¥–µ—Ä–µ–≤—å—è –ø–µ—Ä–µ–æ–±—É—á–∞—é—Ç—Å—è –∏ –∫–∞–∫ —ç—Ç–æ–≥–æ –∏–∑–±–µ–∂–∞—Ç—å?
- –ö–∞–∫ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤?

### Random Forest
- –ë—ç–≥–≥–∏–Ω–≥, bootstrapping
- –£–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏

**–í–æ–ø—Ä–æ—Å—ã:**
- –ß—Ç–æ —Ç–∞–∫–æ–µ –±—ç–≥–≥–∏–Ω–≥?
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å?
- –ü–æ—á–µ–º—É RF —É—Å—Ç–æ–π—á–∏–≤ –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é?
- –ö–∞–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (out-of-bag)?

### Gradient Boosting (XGBoost, LightGBM, CatBoost)
- –ê–¥–¥–∏—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
- –°–ª–∞–±—ã–µ —É—á–µ–Ω–∏–∫–∏
- –õ–æ—Å—Å-—Ñ—É–Ω–∫—Ü–∏–∏ (log-loss, MAE, Huber)

**–í–æ–ø—Ä–æ—Å—ã:**
- –í —á—ë–º –∏–¥–µ—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞?
- –ü–æ—á–µ–º—É —Å–ª–∞–±—ã–µ –º–æ–¥–µ–ª–∏ –≤–∞–∂–Ω—ã?
- –í —á—ë–º –ø–ª—é—Å—ã XGBoost/LightGBM/CB?
- –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Huber –≤–º–µ—Å—Ç–æ MSE?

### KNN
- –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –º–∞—Å—à—Ç–∞–±—É
- KD/ball trees
- –í—ã—Å–æ–∫–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å

**–í–æ–ø—Ä–æ—Å—ã:**
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç KNN?
- –ü–æ—á–µ–º—É –º–∞—Å—à—Ç–∞–± –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∞–∂–µ–Ω?
- –ö–∞–∫ —É—Å–∫–æ—Ä–∏—Ç—å –ø–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π?
- –ü–æ—á–µ–º—É KNN –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ high-dimensional space?

### SVM
- –ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –æ—Ç—Å—Ç—É–ø–∞
- Kernel trick: linear, RBF
- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã C –∏ gamma

**–í–æ–ø—Ä–æ—Å—ã:**
- –í —á—ë–º –∏–¥–µ—è SVM?
- –ó–∞—á–µ–º –Ω—É–∂–µ–Ω kernel trick?
- –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä C?
- –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å —è–¥—Ä–æ?

### Naive Bayes
- –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è —Ç–µ–æ—Ä–µ–º–∞
- –ù–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ NLP

**–í–æ–ø—Ä–æ—Å—ã:**
- –í —á—ë–º —Å—É—Ç—å –Ω–∞–∏–≤–Ω–æ–≥–æ –ë–∞–π–µ—Å–∞?
- –ü–æ—á–µ–º—É –æ–Ω "–Ω–∞–∏–≤–Ω—ã–π"?
- –ö–æ–≥–¥–∞ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ?

### K-Means, K-Means++
- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–Ω—Ç—Ä–æ–≤
- Inertia, Elbow method
- Silhouette score

**–í–æ–ø—Ä–æ—Å—ã:**
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç K-Means?
- –ó–∞—á–µ–º –Ω—É–∂–µ–Ω K-Means++?
- –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤?

### DBSCAN
- eps, min_samples
- –ü–ª–æ—Ç–Ω–æ—Å—Ç–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
- –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–ª–æ—Ç–Ω–æ—Å—Ç—è–º–∏

**–í–æ–ø—Ä–æ—Å—ã:**
- –ß–µ–º DBSCAN –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç K-Means?
- –ß—Ç–æ –æ–∑–Ω–∞—á–∞—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã eps –∏ min_samples?
- –ü–æ—á–µ–º—É DBSCAN –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –ø–ª–æ—Ç–Ω–æ—Å—Ç—è—Ö?

### –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
- L1, L2, Elastic Net

**–í–æ–ø—Ä–æ—Å—ã:**
- –ß–µ–º L1 –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç L2?
- –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Elastic Net?
- –ö–∞–∫ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ?

---

## –ù–µ–π—Ä–æ—Å–µ—Ç–∏

- MLP, CNN, RNN (LSTM, GRU)
- Backpropagation, softmax
- Activation functions: ReLU, tanh, sigmoid, LeakyReLU, GELU
- –ü—Ä–æ–±–ª–µ–º—ã –∑–∞—Ç—É—Ö–∞—é—â–∏—Ö/–≤–∑—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤: He/Xavier
- Loss: BCE, CCE, MSE
- Dropout, BatchNorm, Early stopping
- LR Scheduler
- Transfer learning: Fine-tuning vs Feature extraction

**–í–æ–ø—Ä–æ—Å—ã:**
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç MLP?
- –í —á—ë–º –ø–ª—é—Å—ã CNN?
- –ü–æ—á–µ–º—É LSTM/GRU –ª—É—á—à–µ –æ–±—ã—á–Ω—ã—Ö RNN?
- –ß—Ç–æ —Ç–∞–∫–æ–µ backpropagation?
- –ö–∞–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≥–¥–µ –ª—É—á—à–µ –ø—Ä–∏–º–µ–Ω—è—Ç—å?
- –í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ BCE –∏ CCE?
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç dropout?
- –ß—Ç–æ —Ç–∞–∫–æ–µ transfer learning?

---

## –ú–µ—Ç—Ä–∏–∫–∏

### –†–µ–≥—Ä–µ—Å—Å–∏—è:
- MSE, MAE, RMSE, $R^2$, MAPE, Log loss

### –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:
- Accuracy, Precision, Recall, F1-score
- ROC-AUC, PR-AUC (—Ä–∞–∑–Ω–∏—Ü–∞!)

**–í–æ–ø—Ä–æ—Å—ã:**
- –ß–µ–º MSE –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç MAE?
- –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MAPE?
- –í —á—ë–º —Å–º—ã—Å–ª ROC-AUC –∏ PR-AUC?
- –ö–∞–∫ —Å—á–∏—Ç–∞—Ç—å F1 –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω?

---

## Feature Engineering

- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
- Encodings (OneHot, Label)
- PCA / t-SNE / UMAP
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (median, KNN, —É–¥–∞–ª–µ–Ω–∏–µ)
- –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–º–æ–¥–µ–ª–∏, mutual info, –±–æ—Ä—É—Ç–∞)
- Binning

**–í–æ–ø—Ä–æ—Å—ã:**
- –ó–∞—á–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ?
- –í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ PCA –∏ t-SNE?
- –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏?

---

## –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞

- SMOTE, oversampling, undersampling
- class weights
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Recall / PR-AUC / F1

**–í–æ–ø—Ä–æ—Å—ã:**
- –ö–∞–∫ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤?
- –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SMOTE?
- –ó–∞—á–µ–º –∑–∞–¥–∞–≤–∞—Ç—å class_weight?

---

## Misc

- Bias¬≤ + Variance + Noise: Expected Error
- Train/Val/Test Split
- K-Fold, Stratified CV
- Grid / Random / Bayesian Optimization
- Data Leakage
- Cold Start
- Concept Drift
- –ü—Ä–æ–±–ª–µ–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- Optimizers: SGD, Adam, RMSprop, Adagrad

**–í–æ–ø—Ä–æ—Å—ã:**
- –ß—Ç–æ —Ç–∞–∫–æ–µ Bias-Variance tradeoff?
- –ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–µ–ª–∞—Ç—å K-Fold?
- –ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç data leakage?
- –ß—Ç–æ —Ç–∞–∫–æ–µ concept drift?
- –ß–µ–º Adam –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç SGD?

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–±—ã—Ç–∏—è
–£—Å–ª–æ–≤–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
–¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞
–°–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã:
–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ
–î–∏—Å–ø–µ—Ä—Å–∏—è
–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:
–ë–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
–ü—É–∞—Å—Å–æ–Ω–æ–≤—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
–ó–∞–∫–æ–Ω –±–æ–ª—å—à–∏—Ö —á–∏—Å–µ–ª –∏ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–µ–ª—å–Ω–∞—è —Ç–µ–æ—Ä–µ–º–∞
–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
–í—ã–±–æ—Ä–æ—á–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
–í—ã–±–æ—Ä–æ—á–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
–í—ã–±–æ—Ä–æ—á–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
–î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–ø–æ—Ç–µ–∑:
–ù—É–ª–µ–≤–∞—è –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≥–∏–ø–æ—Ç–µ–∑—ã
–ö—Ä–∏—Ç–µ—Ä–∏–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ (p-value)
–†–µ–≥—Ä–µ—Å—Å–∏—è:
–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
–ú–µ—Ç–æ–¥ –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤
–ö—Ä–∏—Ç–µ—Ä–∏–∏ —Å–æ–≥–ª–∞—Å–∏—è:
–ö—Ä–∏—Ç–µ—Ä–∏–π —Ö–∏-–∫–≤–∞–¥—Ä–∞—Ç