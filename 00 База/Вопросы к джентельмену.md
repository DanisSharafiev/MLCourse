# –ù–∞–±–æ—Ä –î–∂–µ–Ω—Ç–ª—å–º–µ–Ω–∞ –ø–æ ML üòà

## –û—Å–Ω–æ–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã

### Linear Regression
- MSE, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ (Normal Equation), –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤
- –ü—Ä–æ–±–ª–µ–º–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏



**–í–æ–ø—Ä–æ—Å—ã:**
- –ß—Ç–æ —Ç–∞–∫–æ–µ MSE –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω?
- –ö–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏?
- –ö–æ–≥–¥–∞ —Å—Ç–æ–∏—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ –≤–º–µ—Å—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–≥–æ —É—Ä–∞–≤–Ω–µ–Ω–∏—è?
- –ö–∞–∫ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –≤ –ª–∏–Ω–µ–π–Ω–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏?
- –ß—Ç–æ —Ç–∞–∫–æ–µ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç—å –∏ –∫–∞–∫ —Å –Ω–µ–π –±–æ—Ä–æ—Ç—å—Å—è?
 
### Logistic Regression
- –°–∏–≥–º–æ–∏–¥–∞, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è
- Cross-entropy
- –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π
- –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ –≤–µ—Å–∞

**–í–æ–ø—Ä–æ—Å—ã:**
- –í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –ª–∏–Ω–µ–π–Ω–æ–π –∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π?
- –ü–æ—á–µ–º—É –≤ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–∏–≥–º–æ–∏–¥–∞?
- –ß—Ç–æ —Ç–∞–∫–æ–µ –∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è?
- –ö–∞–∫ –ª–æ–≥—Ä–µ–≥ —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏?

### üîπ Decision Tree
- –ö—Ä–∏—Ç–µ—Ä–∏–∏: Gini, Entropy
- Overfitting, Pruning
- Feature Importance
- Out-of-bag (–≤ RF)

**–í–æ–ø—Ä–æ—Å—ã:**
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –¥–µ—Ä–µ–≤–æ —Ä–µ—à–µ–Ω–∏–π?
- –ß—Ç–æ –≤—ã–±—Ä–∞—Ç—å ‚Äî Gini –∏–ª–∏ Entropy?
- –ö–∞–∫ –¥–µ—Ä–µ–≤—å—è –ø–µ—Ä–µ–æ–±—É—á–∞—é—Ç—Å—è –∏ –∫–∞–∫ —ç—Ç–æ–≥–æ –∏–∑–±–µ–∂–∞—Ç—å?
- –ö–∞–∫ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤?

### üîπ Random Forest
- –ë—ç–≥–≥–∏–Ω–≥, bootstrapping
- –£–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏

**–í–æ–ø—Ä–æ—Å—ã:**
- –ß—Ç–æ —Ç–∞–∫–æ–µ –±—ç–≥–≥–∏–Ω–≥?
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å?
- –ü–æ—á–µ–º—É RF —É—Å—Ç–æ–π—á–∏–≤ –∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—é?
- –ö–∞–∫ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (out-of-bag)?

### üîπ Gradient Boosting (XGBoost, LightGBM, CatBoost)
- –ê–¥–¥–∏—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
- –°–ª–∞–±—ã–µ —É—á–µ–Ω–∏–∫–∏
- –õ–æ—Å—Å-—Ñ—É–Ω–∫—Ü–∏–∏ (log-loss, MAE, Huber)

**–í–æ–ø—Ä–æ—Å—ã:**
- –í —á—ë–º –∏–¥–µ—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞?
- –ü–æ—á–µ–º—É —Å–ª–∞–±—ã–µ –º–æ–¥–µ–ª–∏ –≤–∞–∂–Ω—ã?
- –í —á—ë–º –ø–ª—é—Å—ã XGBoost/LightGBM/CB?
- –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Huber –≤–º–µ—Å—Ç–æ MSE?

### üîπ KNN
- –ß—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –º–∞—Å—à—Ç–∞–±—É
- KD/ball trees
- –í—ã—Å–æ–∫–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å

**–í–æ–ø—Ä–æ—Å—ã:**
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç KNN?
- –ü–æ—á–µ–º—É –º–∞—Å—à—Ç–∞–± –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–∞–∂–µ–Ω?
- –ö–∞–∫ —É—Å–∫–æ—Ä–∏—Ç—å –ø–æ–∏—Å–∫ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π?
- –ü–æ—á–µ–º—É KNN –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ high-dimensional space?

### üîπ SVM
- –ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –æ—Ç—Å—Ç—É–ø–∞
- Kernel trick: linear, RBF
- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã C –∏ gamma

**–í–æ–ø—Ä–æ—Å—ã:**
- –í —á—ë–º –∏–¥–µ—è SVM?
- –ó–∞—á–µ–º –Ω—É–∂–µ–Ω kernel trick?
- –ß—Ç–æ –¥–µ–ª–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä C?
- –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å —è–¥—Ä–æ?

### üîπ Naive Bayes
- –ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è —Ç–µ–æ—Ä–µ–º–∞
- –ù–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
- –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –≤ NLP

**–í–æ–ø—Ä–æ—Å—ã:**
- –í —á—ë–º —Å—É—Ç—å –Ω–∞–∏–≤–Ω–æ–≥–æ –ë–∞–π–µ—Å–∞?
- –ü–æ—á–µ–º—É –æ–Ω "–Ω–∞–∏–≤–Ω—ã–π"?
- –ö–æ–≥–¥–∞ –æ–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç —Ö–æ—Ä–æ—à–æ?

### üîπ K-Means, K-Means++
- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–Ω—Ç—Ä–æ–≤
- Inertia, Elbow method
- Silhouette score

**–í–æ–ø—Ä–æ—Å—ã:**
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç K-Means?
- –ó–∞—á–µ–º –Ω—É–∂–µ–Ω K-Means++?
- –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤?

### üîπ DBSCAN
- eps, min_samples
- –ü–ª–æ—Ç–Ω–æ—Å—Ç–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
- –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–ª–æ—Ç–Ω–æ—Å—Ç—è–º–∏

**–í–æ–ø—Ä–æ—Å—ã:**
- –ß–µ–º DBSCAN –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç K-Means?
- –ß—Ç–æ –æ–∑–Ω–∞—á–∞—é—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã eps –∏ min_samples?
- –ü–æ—á–µ–º—É DBSCAN –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –ø–ª–æ—Ç–Ω–æ—Å—Ç—è—Ö?

### üî∏ –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
- L1, L2, Elastic Net

**–í–æ–ø—Ä–æ—Å—ã:**
- –ß–µ–º L1 –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç L2?
- –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Elastic Net?
- –ö–∞–∫ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ?

---

## üß† –ù–µ–π—Ä–æ—Å–µ—Ç–∏

- MLP, CNN, RNN (LSTM, GRU)
- Backpropagation, softmax
- Activation functions: ReLU, tanh, sigmoid, LeakyReLU, GELU
- –ü—Ä–æ–±–ª–µ–º—ã –∑–∞—Ç—É—Ö–∞—é—â–∏—Ö/–≤–∑—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤: He/Xavier
- Loss: BCE, CCE, MSE
- Dropout, BatchNorm, Early stopping
- LR Scheduler
- Transfer learning: Fine-tuning vs Feature extraction

**–í–æ–ø—Ä–æ—Å—ã:**
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç MLP?
- –í —á—ë–º –ø–ª—é—Å—ã CNN?
- –ü–æ—á–µ–º—É LSTM/GRU –ª—É—á—à–µ –æ–±—ã—á–Ω—ã—Ö RNN?
- –ß—Ç–æ —Ç–∞–∫–æ–µ backpropagation?
- –ö–∞–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≥–¥–µ –ª—É—á—à–µ –ø—Ä–∏–º–µ–Ω—è—Ç—å?
- –í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ BCE –∏ CCE?
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç dropout?
- –ß—Ç–æ —Ç–∞–∫–æ–µ transfer learning?

---

## üìè –ú–µ—Ç—Ä–∏–∫–∏

### –†–µ–≥—Ä–µ—Å—Å–∏—è:
- MSE, MAE, RMSE, $R^2$, MAPE, Log loss

### –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:
- Accuracy, Precision, Recall, F1-score
- ROC-AUC, PR-AUC (—Ä–∞–∑–Ω–∏—Ü–∞!)

**–í–æ–ø—Ä–æ—Å—ã:**
- –ß–µ–º MSE –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç MAE?
- –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MAPE?
- –í —á—ë–º —Å–º—ã—Å–ª ROC-AUC –∏ PR-AUC?
- –ö–∞–∫ —Å—á–∏—Ç–∞—Ç—å F1 –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω?

---

## üõ† Feature Engineering

- –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
- Encodings (OneHot, Label)
- PCA / t-SNE / UMAP
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (median, KNN, —É–¥–∞–ª–µ–Ω–∏–µ)
- –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–º–æ–¥–µ–ª–∏, mutual info, –±–æ—Ä—É—Ç–∞)
- Binning

**–í–æ–ø—Ä–æ—Å—ã:**
- –ó–∞—á–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ?
- –í —á—ë–º —Ä–∞–∑–Ω–∏—Ü–∞ PCA –∏ t-SNE?
- –ö–∞–∫ –≤—ã–±—Ä–∞—Ç—å –≤–∞–∂–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏?

---

## ‚öñÔ∏è –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞

- SMOTE, oversampling, undersampling
- class weights
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Recall / PR-AUC / F1

**–í–æ–ø—Ä–æ—Å—ã:**
- –ö–∞–∫ —Å–ø—Ä–∞–≤–ª—è—Ç—å—Å—è —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º –∫–ª–∞—Å—Å–æ–≤?
- –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å SMOTE?
- –ó–∞—á–µ–º –∑–∞–¥–∞–≤–∞—Ç—å class_weight?

---

## üß© Misc

- Bias¬≤ + Variance + Noise: Expected Error
- Train/Val/Test Split
- K-Fold, Stratified CV
- Grid / Random / Bayesian Optimization
- Data Leakage
- Cold Start
- Concept Drift
- –ü—Ä–æ–±–ª–µ–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- Optimizers: SGD, Adam, RMSprop, Adagrad

**–í–æ–ø—Ä–æ—Å—ã:**
- –ß—Ç–æ —Ç–∞–∫–æ–µ Bias-Variance tradeoff?
- –ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –¥–µ–ª–∞—Ç—å K-Fold?
- –ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç data leakage?
- –ß—Ç–æ —Ç–∞–∫–æ–µ concept drift?
- –ß–µ–º Adam –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç SGD?

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–æ–±—ã—Ç–∏—è
–£—Å–ª–æ–≤–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
–¢–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞
–°–ª—É—á–∞–π–Ω—ã–µ –≤–µ–ª–∏—á–∏–Ω—ã:
–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ
–î–∏—Å–ø–µ—Ä—Å–∏—è
–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:
–ë–∏–Ω–æ–º–∏–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
–ù–æ—Ä–º–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
–ü—É–∞—Å—Å–æ–Ω–æ–≤—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
–ó–∞–∫–æ–Ω –±–æ–ª—å—à–∏—Ö —á–∏—Å–µ–ª –∏ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–µ–ª—å–Ω–∞—è —Ç–µ–æ—Ä–µ–º–∞
–ú–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:
–í—ã–±–æ—Ä–æ—á–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:
–í—ã–±–æ—Ä–æ—á–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
–í—ã–±–æ—Ä–æ—á–Ω–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è
–î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∏–ø–æ—Ç–µ–∑:
–ù—É–ª–µ–≤–∞—è –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≥–∏–ø–æ—Ç–µ–∑—ã
–ö—Ä–∏—Ç–µ—Ä–∏–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ (p-value)
–†–µ–≥—Ä–µ—Å—Å–∏—è:
–õ–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è
–ú–µ—Ç–æ–¥ –Ω–∞–∏–º–µ–Ω—å—à–∏—Ö –∫–≤–∞–¥—Ä–∞—Ç–æ–≤
–ö—Ä–∏—Ç–µ—Ä–∏–∏ —Å–æ–≥–ª–∞—Å–∏—è:
–ö—Ä–∏—Ç–µ—Ä–∏–π —Ö–∏-–∫–≤–∞–¥—Ä–∞—Ç