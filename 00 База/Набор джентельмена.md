 –Ω–∞–±–æ—Ä –¥–∂–µ–Ω—Ç–µ–ª—å–º–µ–Ω–∞ üòà
### –û—Å–Ω–æ–≤–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã:
**Linear Regression** (MSE, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ (Normal Equation), –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫, –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–æ–≤, –ü—Ä–æ–±–ª–µ–º–∞ –º—É–ª—å—Ç–∏–∫–æ–ª–ª–∏–Ω–µ–∞—Ä–Ω–æ—Å—Ç–∏)
**Logistic Regression** (–∫—Ä–æ—Å—Å-—ç–Ω—Ç—Ä–æ–ø–∏—è, —Å–∏–≥–º–æ–∏–¥–∞, –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è, –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤ —á–µ—Ä–µ–∑ –≤–µ—Å–∞, Cross-entropy)
**Decision Tree** (–∫—Ä–∏—Ç–µ—Ä–∏–∏ Gini –∏ —ç–Ω—Ç—Ä–æ–ø–∏—è, overfitting, pruning, Feature Importance, Out-of-bag)
**Random Forest** (–±—ç–≥–≥–∏–Ω–≥, bootstrapping, —É–º–µ–Ω—å—à–µ–Ω–∏–µ –¥–∏—Å–ø–µ—Ä—Å–∏–∏.)
**Gradient Boosting** (–∞–¥–¥–∏—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏, —Å–ª–∞–±—ã–µ —É—á–µ–Ω–∏–∫–∏, XGBoost, LightGBM, CatBoost, –õ–æ—Å—Å-—Ñ—É–Ω–∫—Ü–∏–∏ (log-loss, MAE, Huber))
**KNN** (–ü—Ä–æ—Å—Ç–æ—Ç–∞, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ –º–∞—Å—à—Ç–∞–±—É, –¥–æ—Ä–æ–≥ –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏, —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ —à—É–º—É, KD-–¥–µ—Ä–µ–≤—å—è / Ball-–¥–µ—Ä–µ–≤—å—è, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –ø—Ä–∏ –≤—ã—Å–æ–∫–∏—Ö —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—è—Ö)
**SVM** (–ú–∞–∫—Å–∏–º–∏–∑–∞—Ü–∏—è –æ—Ç—Å—Ç—É–ø–∞, —è–¥—Ä–∞ (linear, RBF), —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –≤—ã–±–æ—Ä—É C –∏ kernel trick)
**Naive Bayes** (–ë–∞–π–µ—Å–æ–≤—Å–∫–∞—è —Ç–µ–æ—Ä–µ–º–∞, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –ø—Ä–∏–º–µ–Ω–∏–º –∫ —Ç–µ–∫—Å—Ç—É.)
**K-Means**, **K-Means++** (Inertia, Elbow method, Silhouette score.)
**DBSCAN** (- –ü–ª–æ—Ç–Ω–æ—Å—Ç–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è, eps, min_samples, –ø—Ä–æ–±–ª–µ–º—ã —Å –∏–∑–º–µ–Ω–µ–Ω–∏–µ–º –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏.)
–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è: L1, L2, Elastic net.

### –ù–µ–π—Ä–æ—Å–µ—Ç–∏
MLP 
CNN
RNN (LSTM, GRU (—Ö–æ—Ç—è –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ, –Ω–æ —Å—Ç–æ–∏—Ç –∑–Ω–∞—Ç—å).)
Backpropagation, softmax, 
Activation functions (**ReLU, tanh, sigmoid, LeakyReLU, GELU**).
–ü—Ä–æ–±–ª–µ–º—ã –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –∑–∞—Ç—É—Ö–∞–Ω–∏—è/–≤–∑—Ä—ã–≤–∞
–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤ (He/Xavier).
Loss functions: Binary Crossentropy, Categorical Crossentropy, MSE
–†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è: Dropout, Batch Normalization, Early stopping.
LR Scheduler,
Transfer learning,
Fine-tuning vs Feature extraction,

### –ú–µ—Ç—Ä–∏–∫–∏
–†–µ–≥—Ä–µ—Å—Å–∏—è:
MSE
MAE
RMSE
$R^{2}$
MAPE
Log loss
Confusion matrix

–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è:
**Accuracy**
**Precision**
**Recall**
**F1-score**
**ROC-AUC, PR-AUC** –†–∞–∑–Ω–∏—Ü–∞

### Feature Engineering
–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è, –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
Encodings
PCA/TSNE/UMAP
–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤
–í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
Binning
### –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞
SMOTE, undersampling, oversampling
**class weights** –≤ –º–æ–¥–µ–ª–∏
–ú–µ—Ç—Ä–∏–∫–∏ (**Recall**, **F1**, **PR-AUC**)

### Misc
$\text{Expected Error} = Bias^{2} + Variance + \text{Irreducible error (noise)}$ (–ß—Ç–æ –∏ –∫–æ–≥–¥–∞ –∑–Ω–∞—á–∏—Ç)
**Train/Val/Test split**
**K-Fold Cross Validation**
**Stratified CV**
**Grid search, Random search, Bayesian Optimization**
**Leakage** 
**Cold start**
**Concept drift**
–ü—Ä–æ–±–ª–µ–º—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä—ã (**SGD, Adam, RMSprop, Adagrad**)






