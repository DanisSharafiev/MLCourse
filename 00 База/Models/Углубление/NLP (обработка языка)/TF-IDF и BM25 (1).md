# TF-IDF (Term Frequency, Inverse Document Frequency)

Этот алгоритм оценивает важность слова для конкретного документа среди всей коллекции (множества документов)

TF - Term Frequency, частота термина в документе
IDF - Inverse Document Frequency, обратная частота в документе

Начнем с TF
Оно показывает, насколько часто слово встречается внутри текущего документа.
Вычисляется по следующей формуле:
$$
TF(t,d) = \frac{\text{кол-во вхождений слова t в документе d}}{\text{общее число слов в документе d}}
$$
Здесь можно заметить, что TF будет выше, если слово много раз повторяется в одном документе, значит оно может быть ценным. Но для того, чтобы исключить 

$$
IDF(t) = \log \left( \frac{N}{1 + n_{i}} \right)
$$

где $N$ - общее количество документов,
$n_{i}$ - количество документов, в которых встречалось слово t.
+1 чтобы не было деления на 0.

И конечная формула это просто произведение двух элементов:
$$
\text{TF-IDF}(t, d) = TF(t,d) * IDF(t)
$$
Чем выше TF-IDF $\to$ тем важнее слово для этого документа.

В чем плюсы:
- Дает вес важным словам

Если слово появляется во всех файлах, то, скорее всего, оно не имеет большой ценности, поэтому IDF будет стремиться к 0. В то же время если слово появляется редко в документе, то TF тоже будет возле 0, когда определена в диапазоне \[0, 1].

- Можно сравнивать векторами (в основном cosine similarity)

- Слова с высоким TF-IDF - хорошие ключи.

С последним можно чуть подробнее, т.к. я считаю это довольно интересным. Если отсортировать все слова по TF-IDF, то мы можем получить ключевые слова текста. Представим, что мы прогнали документ с статьей об ML, и в нем ключевые слова это "машинное", "обучение", "ML" и т.п. Эти слова частые в контексте нашего документа и особо не встречаются в других, поэтому они имеют большой вес.

Но есть и минусы:
- Не учитывает порядок слов
- Не понимает синонимы
- Плохо работает с короткими текстами
- Дает разряженные вектора
# BM25

Разработана для систем ранжирования документов по поиску

$$
\text{BM25}(Q, D) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{\text{TF}(q_i, D) \cdot (k_1 + 1)}{\text{TF}(q_i, D) + k_1 \cdot \left(1 - b + b \cdot \frac{|D|}{\text{avgdl}}\right)}

$$

- $Q = \{q_1, q_2, \dots, q_n\}$ — запрос, представленный как набор терминов.
- $D$ — документ, для которого вычисляется релевантность.
- $|D|$ — длина документа $D$ (количество слов).
- $\text{avgdl}$ — средняя длина документов в коллекции.
- $k_1$ и $b$ — свободные параметры, которые настраиваются (обычно $k_1 \in [1.2, 2.0]$, $b \in [0.5, 0.75]$).

Плюсы: учитывает длину документа, есть гибкая настройка параметров для значимости, более точное ранжирование в поисковых системах

