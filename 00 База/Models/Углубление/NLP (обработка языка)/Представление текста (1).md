# BoW (Bag of words)

Создает словарь в виде:
- Ключ - слово
- Значение -  количество этого слова в тексте

Пример:
``` python
"Я люблю и папу, и маму"
```

Создаем словарь с словами:
``` python
["Я", "люблю", "и", "папу", "маму"]
```

где BoW-вектор это
``` python
[1, 1, 2, 1, 1]
```

Очень простой и быстро обучается, но не учитывает: 
- порядок слов, 
- значение (пес и собака разные индексы, хотя близкие по значению), 
- может быть много нулей, что создает проблему разряженностью (sparse). Способен занимать много места, работать медленно без оптимизаций, очень большая размерность.

Предобрабатывают обычно так:
- Удаление спец символов, разметки
- Приведение к нижнему регистру
- Удаление стоп слов: "и", "в", "на", "это"

# TF-IDF (Term Frequency, Inverse Document Frequency)

Этот алгоритм оценивает важность слова для конкретного документа среди всей коллекции (множества документов)

TF - Term Frequency, частота термина в документе
IDF - Inverse Document Frequency, обратная частота в документе

Начнем с TF
Оно показывает, насколько часто слово встречается внутри текущего документа.
Вычисляется по следующей формуле:
$$
TF(t,d) = \frac{\text{кол-во вхождений слова t в документе d}}{\text{общее число слов в документе d}}
$$
Здесь можно заметить, что TF будет выше, если слово много раз повторяется в одном документе, значит оно может быть ценным. Но для того, чтобы исключить 

$$
IDF(t) = \log \left( \frac{N}{1 + n_{i}} \right)
$$

где $N$ - общее количество документов,
$n_{i}$ - количество документов, в которых встречалось слово t.
+1 чтобы не было деления на 0.

И конечная формула это просто произведение двух элементов:
$$
\text{TF-IDF}(t, d) = TF(t,d) * IDF(t)
$$
Чем выше TF-IDF $\to$ тем важнее слово для этого документа.

В чем плюсы:
- Дает вес важным словам

Если слово появляется во всех файлах, то, скорее всего, оно не имеет большой ценности, поэтому IDF будет стремиться к 0. В то же время если слово появляется редко в документе, то TF тоже будет возле 0, когда определена в диапазоне \[0, 1].

- Можно сравнивать векторами (в основном cosine similarity)

- Слова с высоким TF-IDF - хорошие ключи.

С последним можно чуть подробнее, т.к. я считаю это довольно интересным. Если отсортировать все слова по TF-IDF, то мы можем получить ключевые слова текста. Представим, что мы прогнали документ с статьей об ML, и в нем ключевые слова это "машинное", "обучение", "ML" и т.п. Эти слова частые в контексте нашего документа и особо не встречаются в других, поэтому они имеют большой вес.

Но есть и минусы:
- Не учитывает порядок слов
- Не понимает синонимы
- Плохо работает с короткими текстами
- Дает разряженные вектора

# Word Embeddings

Способ представить слово как вектор чисел, чтобы модель понимала смысл слова, а не просто рандомный IDшник. Это плотные вектора с семантикой.

Схожие слова должны иметь приблизительно схожие вектора, похожесть которых можно оценить с помощью cosine similarity.

Например:
KFC -> \[0.85, 0.24, 0.3]
McDonalds -> \[0.83, 0.22, 0.29]

## Word2Vec

Если слово встречается рядом с другими словами - значит, они связаны.

Разделяют на два типа:
- CBOW (Continuous Bag of Words): предсказываем слово по контексту
  программист, в, кофейне -> программирует
- Skip-gram: по слову предсказываем окружение
  программирует -> программист, в, кофейне

На каждое слово сопоставляется свой вектор с фикс. длинной.

![Word2vec](https://raw.githubusercontent.com/DanisSharafiev/MLCourse/refs/heads/main/Images/Pastedimage20250606122729.png)

Есть некоторые проблемы:
Не учитывает глобальный контекст (решает GloVe),
Плохо работает с морфологически богатыми языками (решает FastTEXT)


## GloVe

GloVe реша
