# BoW (Bag of words)

Создает словарь в виде:
- Ключ - слово
- Значение -  количество этого слова в тексте

Пример:
``` python
"Я люблю и папу, и маму"
```

Создаем словарь с словами:
``` python
["Я", "люблю", "и", "папу", "маму"]
```

где BoW-вектор это
``` python
[1, 1, 2, 1, 1]
```

Очень простой и быстро обучается, но не учитывает: 
- порядок слов, 
- значение (пес и собака разные индексы, хотя близкие по значению), 
- может быть много нулей, что создает проблему разряженностью (sparse). Способен занимать много места, работать медленно без оптимизаций, очень большая размерность.

Предобрабатывают обычно так:
- Удаление спец символов, разметки
- Приведение к нижнему регистру
- Удаление стоп слов: "и", "в", "на", "это"

# TF-IDF (Term Frequency, Inverse Document Frequency)

Этот алгоритм оценивает важность слова для конкретного документа среди всей коллекции (множества документов)

TF - Term Frequency, частота термина в документе
IDF - Inverse Document Frequency, обратная частота в документе

Начнем с TF
Оно показывает, насколько часто слово встречается внутри текущего документа.
Вычисляеттся по следующей формуле:
$$
TF(t,d) = \frac{\text{кол-во вхождений слова t в документе d}}{\text{общее число слов в документе d}}
$$

$$
IDF(t) = \log \left( \frac{N}{1 + n_{i}} \right)
$$

где $N$ - общее количество документов,
$n_{i}$ - количество документов, в которых встречалось слово t.
+1 чтобы не было деления на 0.

