Типы проблем
	Какие алгоритмы стоит прям по названиям и преимущества

### Типы проблем

Для начала объясню какие основные задачи решает ML.
- Классификация - предсказание класса (является ли человек медийной личностью в форме да или нет).
- Регрессия - предсказание числа (цена дома).
- Кластеризация - нахождение закономерностей у данных и объединение в группы.
- Ранжирование - упорядочивание объектов по релевантности.
- Снижение размерности - упрощение данных.
- Обработка естественного языка (NLP) - анализ и генерация структуры.
- Компьютерное зрение (CV) - обработка изображений/видео.
- Прогнозирование временных рядов - предсказание на основе временных данных.
- Обучение с подкреплением - Оптимизация действий в среде.

Более менее должно быть уже понятно, сейчас будем соотносить разные алгоритмы к определенным задачам.

## Классификация

##### Логистическая регрессия (Logistic regression):
В основном используют для простых бинарных классификаций по типу кредитного скоринга (просто пример).
Простая, интерпретируемая, быстрая. Но из-за того, что простая - не может работать с сложными зависимостями.

##### Дерево решений (Decision tree):
Можно использовать для классификации клиентов, диагностики заболеваний.
Интерпретируемая, работает с категориями, но легко переобучается, нужно за этим следить.

##### Случайный лес (Random Forest):
Можно использовавть для классификации изображений.
Метод Bagging. Устойчив к шуму и имеет высокую точность, но медленный на больших данных.

##### Градиентный бустинг (XGBoost)
Подходит для классификации транзакций (фрод).
Имеет высокую точность, гибкость, но необходимо подбирать гиперпараметры.

##### Градиентный бустинг (LightGBM)
Классификация больших данных (интернет-реклама).
Быстрый, эффективный на больших данных, но хуже интерпретируется.

##### Градиентный бустинг (Catboost)
Классификация с категориальными данными, подходит для рекомендационных систем.
Удобнее для категориальных переменных, устойчив к переобучению, но дольше обучается, чем LightGBM.

##### K-ближайших соседей (KNN)
Подходит для рекомендационных систем.
Простой, не требует обучения, но медленный и чувствителен к шуму.

##### Наивный Байес (Naive bayes)
Детекция спама, анализ текстов (классификация настроений).
Быстрый, хорош для текстов, но не учитывает порядок слов.

##### Машина опорных векторов (SVM)
Классификация гиперплоскостью.
Эффективен в высокомерных пространствах, но если много данных - медленный.

##### MLP
Можно переделать под любые задачи.
Понимает сложные зависимости, но требует много данных и вычислений.

##### CNN
Классификация изображений.
Хорошо классифицирует, сложная в настройке.

##### RNN
Классификация текстов
Эффективен для последовательностей, проблемы с долгосрочными зависимостями.

##### Градиентный бустинг (Adaboost)
Неплох в распознавании лиц, классификации клиентов.
Устойчив к шуму, но чувствителен к выбросам.

##### Линейный дискриминантный анализ (LDA)
Классификация в биометрии
Снижает размерность, но предполает нормальных данных

##### ResNet
Классификация сложных изображений (медицинская диагностика)
Имеет высокую точность, но требует больших данных и ресурсов.

## Регрессия

##### Линейная регрессия
Прогнозирует цены (недвижимость).
Простая, интерпретируемая, но не моделирует нелинейности.
Сюда же входит Ridge, Lasso, ElasticNet, где Ridge - устойчива к мультиколлинеарности, Lasso - удаляет признаки, а ElasticNet - Lasso + Ridge.

##### Дерево решений (Decision tree):
Прогнозирует продажи, оценивает риски.
Интерпретируемая, моделирует нелинейности, но легко переобучается, нужно за этим следить.

##### Случайный лес (Random Forest):
Предсказывает погоду, оценивает недвижимость.
Метод Bagging. Устойчив к шуму и имеет высокую точность, но медленный на больших данных.

##### Градиентный бустинг (XGBoost)
Подходит для прогноза спроса, кредитный скоринг.
Имеет высокую точность, гибкость, но необходимо подбирать гиперпараметры.

##### Градиентный бустинг (LightGBM)
Прогноз больших данных (онлайн-реклама)
Быстрый, эффективный на больших данных, но хуже интерпретируется.

##### Градиентный бустинг (Catboost)
Прогноз с категориями (рекомендации).
Удобнее для категориальных переменных, устойчив к переобучению, но дольше обучается, чем LightGBM.

##### K-ближайших соседей (KNN)
Подходит для рекомендационных систем, прогнозирует цены.
Простой, не требует обучения, но медленный и чувствителен к шуму.

##### Машина опорных векторов (SVM)
Прогнозирует сложные данные (финансовые рынки)
Эффективен в высокомерных пространствах, но если много данных - медленный.

##### MLP
Можно переделать под любые задачи.
Понимает сложные зависимости, но требует много данных и вычислений.

##### CNN
Прогноз на изображениях (оценка ущерба).
Хорошо работает в пространственных данных (изображения, например), сложная в настройке.

##### Bayesian Regression
Прогнозирует с неопределенностью (медицина).
Учитывает неопределенность, но вычислительно сложна.

##### Гауссовские процессы
Прогноз с доверительными интервалами (калибровка моделей).
Гибкие процессы, дают доверительные интервалы, но плохо масштабируются.

## Кластеризация

## Ранжирование

## Снижение размерности

## Прогнозирование временых рядов

## Компьютерное зрение

## Обработка естественного языка

## Обучение с подкреплением






























