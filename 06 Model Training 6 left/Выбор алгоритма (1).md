Типы проблем
	Какие алгоритмы стоит прям по названиям и преимущества

### Типы проблем

Для начала объясню какие основные задачи решает ML.
- Классификация - предсказание класса (является ли человек медийной личностью в форме да или нет).
- Регрессия - предсказание числа (цена дома).
- Кластеризация - нахождение закономерностей у данных и объединение в группы.
- Ранжирование - упорядочивание объектов по релевантности.
- Снижение размерности - упрощение данных.
- Обработка естественного языка (NLP) - анализ и генерация структуры.
- Компьютерное зрение (CV) - обработка изображений/видео.
- Прогнозирование временных рядов - предсказание на основе временных данных.
- Обучение с подкреплением - Оптимизация действий в среде.

Более менее должно быть уже понятно, сейчас будем соотносить разные алгоритмы к определенным задачам.

## Классификация

##### Логистическая регрессия (Logistic regression):
В основном используют для простых бинарных классификаций по типу кредитного скоринга (просто пример).
Простая, интерпретируемая, быстрая. Но из-за того, что простая - не может работать с сложными зависимостями.

##### Дерево решений (Decision tree):
Можно использовать для классификации клиентов, диагностики заболеваний.
Интерпретируемая, работает с категориями, но легко переобучается, нужно за этим следить.

##### Случайный лес (Random Forest):
Можно использовавть для классификации изображений.
Метод Bagging. Устойчив к шуму и имеет высокую точность, но медленный на больших данных.

##### Градиентный бустинг (XGBoost)
Подходит для классификации транзакций (фрод).
Имеет высокую точность, гибкость, но необходимо подбирать гиперпараметры.

##### Градиентный бустинг (LightGBM)
Классификация больших данных (интернет-реклама).
Быстрый, эффективный на больших данных, но хуже интерпретируется.

##### Градиентный бустинг (Catboost)
Классификация с категориальными данными, подходит для рекомендационных систем.
Удобнее для категориальных переменных, устойчив к переобучению, но дольше обучается, чем LightGBM.

##### K-ближайших соседей (KNN)
Подходит для рекомендационных систем.
Простой, не требует обучения, но медленный и чувствителен к шуму.

##### Наивный Байес (Naive bayes)
Детекция спама, анализ текстов (классификация настроений).
Быстрый, хорош для текстов, но не учитывает порядок слов.

##### Машина опорных векторов (SVM)
Классификация гиперплоскостью.
Эффективен в высокомерных пространствах, но если много данных - медленный.

##### MLP
Можно под любые задачи переделать
Понимает сложные зависимости, но требует много данных и вычислений.

##### CNN
Классификация изображений.
Хорошо классифицирует, сложная в настройке.

##### RNN
Классификация текстов
Эффективен для последовательностей, проблемы с долгосрочными зависимостями.

##### Градиентный бустинг (Adaboost)
Неплох в распознавании лиц, классификации клиентов.
Устойчив к шуму, но чувствителен к выбросам.

##### Линейный дискриминантный анализ (LDA)
Классификация в биометрии
Снижает размерность, но предполает нормальных данных

##### ResNet
Классификация сложных изображений (медицинская диагностика)
Имеет высокую точность, но требует больших данных и ресурсов.

## Регрессия


## Кластеризация

## Ранжирование

## Снижение размерности

## Прогнозирование временых рядов

## Компьютерное зрение

## Обработка естественного языка

## Обучение с подкреплением






























