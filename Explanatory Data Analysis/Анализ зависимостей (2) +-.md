Здесь затронем то, как переменные зависят друг от друга (естественно, преимущественно графики). Надо всегда понимать, что переменные могут зависеть друг от друга линейно ($y = 2x$) и нелинейно($y = x^{2}$), кроме этого несколько признаков способны идеально предсказать значение, когда по отдельности они могут быть слабо зависимы от таргета (условно если $y = x_{1} + x_{2}$, мы не сможем предсказать $y$ только по $x_{1}$).

Импортируем библиотеки и создадим датафрейм:

``` python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
  
data = pd.DataFrame({
    "X": [1, 2, 3, 4, 5, 6],
    "Y": [2, 4, 6, 8, 10, 12],
    "Z": [10, 9, 7, 6, 4, 2],
    "W": [1, 4, 9, 16, 25, 36],
    "square": [-5, 1, 2, 2, 1, -5]
})
```

И нарисуем тепловую карту корреляций:

``` python
sns.heatmap(data.corr(), annot=True, cmap="coolwarm")
plt.show()
```

![Heatmap](https://raw.githubusercontent.com/DanisSharafiev/MLCourse/refs/heads/main/Images/11.png)

Разберем для начала аргументы: 
1) data.corr() Мы передаем уже готовую матрицу корреляций
2) annot = True Включает отображение значений
3) Задаем цветовую палитру (синий -> серый -> красный)
4) можно еще задать`method='pearson'` или `method='spearman'` в data.corr()

Это изображение показывает связь между двумя переменными, по умолчанию стоит pearson, данный метод описывает линейную зависимость, когда spearman работает благодаря монотонности. Если значение близко к 1 или -1, оно довольно хорошо показывает. Если близко к 0, то данные для данного метода выглядят как случайные. Иногда приходится отбирать какие-то признаки, поэтому эта карта может помочь.

Можно еще отобразить в виде scatter plot. Самостоятельно можно находить и линейные и нелинейные зависимости:

``` python
fig, axes = plt.subplots(1, 3, figsize=(15, 5))
  
axes[0].scatter(data['X'], data['Y'], label='(X, Y)', color='blue')
axes[0].set_title('(X, Y)')
axes[0].set_xlabel('X')
axes[0].set_ylabel('Y')
  
axes[1].scatter(data['X'], data['Z'], label='(X, Z)', color='green')
axes[1].set_title('(X, Z)')
axes[1].set_xlabel('X')
axes[1].set_ylabel('Z')
  
axes[2].scatter(data['X'], data['square'], label='(X, square)', color='red')
axes[2].set_title('(X, square)')
axes[2].set_xlabel('X')
axes[2].set_ylabel('square')
  
plt.show()
```

![Plots](https://raw.githubusercontent.com/DanisSharafiev/MLCourse/refs/heads/main/Images/12.png)

Тут явно видно, что X зависит от Y линейно, X и Z похожая ситуация, но есть небольшие погрешности, а X от square зависит нелинейно, в виде параболы.

Ну и можно отдельно при необходимости повыводить значения по пирсону:

``` python
data["X"].corr(data["Y"]) # 1.0
```

``` python
data["X"].corr(data["square"]) # -1.4019986669674834e-17
```

И по спирмену:

``` python
corr1 = data["X"].corr(data["Y"], method='spearman')
corr2 = data["X"].corr(data["square"], method='spearman')
print(corr1, corr2) # 1.0 0.0
```

Анализ вариативности