Этот метод об оценке вклада каждого признака в предсказания модели.
Здесь можно выделить несколько основных подходов:

- На основе уменьшения ошибки:
  Для каждого признака измеряется, насколько уменьшается ошибка (например, MSE или Gini) при его использовании для разбиения в дереве. Признаки, которые чаще используются для разбиения и сильнее уменьшают ошибку, считаются более важными. Используют преимущественно в бустинге, т.к. легче отслеживать ошибку.
  
- На основе перестановок 
  Для каждого признака измеряется, насколько ухудшается качество модели, если значения этого признака случайно переставить. Чем сильнее ухудшение, тем важнее признак. Особенно популярен для нейросетей: он простой, работает с любой нейросетью.
  
- На основе весов
  В линейных моделях важность признака может быть оценена по абсолютному значению его коэффициента. 

Пример для случайного леса:
- Для каждого дерева вычисляется важность признаков на основе уменьшения ошибки.
- Итоговая важность признака — это среднее значение по всем деревьям.

Покажу как обычно получают в виде кода (Но скорее абстрактно, опущу ненужное для примера):
``` python
from xgboost import XGBClassifier
from sklearn.datasets import load_iris

data = load_iris()
X = data.data
y = data.target

model = XGBClassifier()
model.fit(X, y)

importances = model.feature_importances_
```

Если вывести importances, увидим следующее:
``` console
[0.00959796 0.01645038 0.6765859 0.2973658]
```

Чем больше значение - тем больше ценится признак.