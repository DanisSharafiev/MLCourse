Для примера создадим простой датафрейм:
``` python
import pandas as pd
a = [1, 2, 3]
df = pd.DataFrame(a)
```
### Standard Scaler
Разберем сначала Standard Scaler.

Формула следующая:
$X' = \frac{X- \mu}{\sigma}$
Она должна быть знакома с курса статистики.

Переводит все данные так, чтобы среднее по ним было равно нулю и стандартное отклонение было равно 1. Используется для нормальных распределений, когда алгоритмы чувствительны к масштабу, хотим сохранить статистические свойства данных.

Из недостатков, чувствительный к выбросам.

``` python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = scaler.fit_transform(df)
df_scaled
```

``` console
array([[-1.60356745], 
	   [-0.26726124], 
	   [ 1.06904497], 
	   [-0.26726124], 
	   [ 1.06904497]])
```

### Min-Max Scaler

Используем, когда нужен диапазон от 0 до 1. Помимо этого, можем сделать собственные диапазоны: если умножим на 2 и вычтем 1 из всех данных, получим диапазон от -1 до 1. Требует алгоритмы чувствительные к абсолютным значениям. Также не требует нормальное распределение.

Формула:
$X' = \frac{X - X_{min}}{X_{max}-X_{min}}$
Ну тут все логично, если из самого минимального значения из выборки вычесть его же, получим 0, меньше нуля не получится, т.к. мы и так уже взяли самое минимальное значение (Прошу заметить, проблема с негативными числами решена: минус на минус дает плюс и мы получаем все значения в положительной оси).
Ну и делим все на новое максимальное значение (Поделить самое больше само на себя - единица).

``` python
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_scaled = scaler.fit_transform(df)
df_scaled
```

``` console
array([[0. ], 
	   [0.5], 
	   [1. ], 
	   [0.5], 
	   [1. ]])
```

### Robust scaler

Если в данных есть выбросы, можно использовать, т.к. он использует использует медиану и квартили (Насчет этого уже объяснял ранее в EDA Поиск выбросов). Не нормальные распределения(Ассиметрия или тяжелые хвосты) или алгоритмы чувствительные к выбросам (На основе деревьев решений, PCA).

Формула:
$X' = \frac{X-median}{IQR}$

Может быть широкий диапазон, из-за чего алгоритмы, где требуют фиксированный - чувствуют себя хуже.

``` python
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
df_scaled = scaler.fit_transform(df)
df_scaled
```

``` console
array([[-1.], 
       [ 0.], 
       [ 1.], 
       [ 0.], 
       [ 1.]])
```

### Как делают на практике:

Во-первых, не применяют на target. Во-вторых, при разделении на тренировочную и тестовую выборки обучают исключительно на тренировочной, после чего применяют на новые данные (в нашем случае тестовая выборка).

Пример (Представьте какой-то датафрейм себе):
``` python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

