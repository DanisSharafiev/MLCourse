Если у нас есть дубликаты, то стоит от них избавиться, они способны искажать данные, а именно статистику, модели могут переобучиться, еще легко возникают ошибки с восприятием и анализом данных. Помимо этого возможно ускорение вычислений.

Создадим датасетик мега простенький

``` python
import pandas as pd

df = pd.DataFrame([{"name" : "Artem", "age" : 25, "city" : "Moscow"},
      {"name" : "Dima", "age" : 23, "city" : "Kiev"},
      {"name" : "Artem", "age" : 25, "city" : "Moscow"},
      {"name" : "Vasya", "age" : 25, "city" : "Minsk"}])
```

Самое простое: удаляет 100% совпадающие примеры.

``` python
df.drop_duplicates()
```
Output:
``` python
  name age city 
0 Artem 25 Moscow 
1 Dima  23 Kiev 
3 Vasya 25 Minsk
```

Можно удалять повторяющиеся по определенному признаку:

``` python
df.drop_duplicates(subset = ["age"])
```
Output:
``` python
  name age city 
0 Artem 25 Moscow 
1 Dima  23 Kiev
```

Еще можно оставлять последнее (по умолчанию первое) вхождение.

``` python
df.drop_duplicates(subset=["age"], keep='last')
```
Output:
``` python
  name age city 
1 Dima  23 Kiev 
3 Vasya 25 Minsk
```

И еще можно удалять в целом все дублирующиеся строки:

``` python
df.drop_duplicates(keep=False)
```
Output:
``` python
  name age city 
1 Dima  23 Kiev 
3 Vasya 25 Minsk
```
